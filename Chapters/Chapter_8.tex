\chapter{Differentialrechnung in $\mathbb{R}^n$}
\section{Partielle Ableitungen und Differential}
Wie kann man die Begriffe der \todo{Missing content?? page 113 top} Differentialrechnung auf Funktionen $f:\Omega \subset \mathbb{R}^n\rightarrow\mathbb{R}$ erweitern?\\

Funktion in mehreren variablen sind ein bisschen komplizierter als Funktionen in einer variable.
\subsubsection*{Beispiel}
\begin{enumerate}
\item $f(x)=x^2+5$ ist in ursprung stetig da $\lim\limits_{x\rightarrow 0}f(x)=f(0)$. Aber $f:\mathbb{R}^2\rightarrow\mathbb{R}$ \[f(x,y) = \left\{ {\begin{array}{*{20}{c}}
{\frac{{xy}}{{{x^2} + {y^2}}}}&{(x,y)\not  = (0,0)}\\
0&{(x,y) = (0,0)}
\end{array}} \right.\] ist im Ursprung nicht stetig. %page 114 top
\todo{Where is number 2 of the beispiel??}
\end{enumerate}
\todo{is this continuation of the Beispiel, or is it outside??}
\begin{figure}[ht]
\begin{minipage}[b]{0.45\linewidth}
\centering
\[\mathop {\lim }\limits_{\begin{array}{*{20}{c}}
{x \to 0}\\
{y = 0}
\end{array}} \frac{{x \cdot y}}{{{x^2} + {y^2}}} = 0 = f(0,0)\]
\end{minipage}
\hspace{0.5cm}
\begin{minipage}[b]{0.45\linewidth}
\centering
\[\mathop {\lim }\limits_{\begin{array}{*{20}{c}}
{y \to 0}\\
{x = 0}
\end{array}} \frac{{x \cdot y}}{{{x^2} + {y^2}}} = 0 = f(0,0)\]
\end{minipage}
\end{figure}

Aber der Limes entlang der Gerade $y=mx$ 
\[\mathop {\lim }\limits_{\begin{array}{*{20}{c}}
{x \to 0}\\
{y \to 0}\\
{y = mx}
\end{array}} f(x,mx) = \mathop {\lim }\limits_{x \to 0} \frac{{m{x^2}}}{{(1 + {m^2}){x^2}}} = \mathop {\frac{m}{{1 + {m^2}}}}\limits_{\begin{array}{*{20}{c}}
 \downarrow \\
{{\text{Hängt von }} m {\text{ ab}}}
\end{array}} \]
und $\frac{m}{1+m^2}\not=0$, falls $m\not=0$. Eine funktion $f(x,y)$ an der stelle $(x_0,y_0)$ ist stetig wenn der limes $\mathop {\lim }\limits_{(x,y) \to ({x_0},{y_0})} f(x,y)$ in jeder Richtung der gleichen wert haben. 
\begin{definition}{8.1}
Sei $\Omega\subset\mathbb{R}^n$, $f:\Omega \rightarrow\mathbb{R}$, $a\in\Omega$
\begin{enumerate}
\item $f$ hat den Grenzwert $c\in\mathbb{R}$, d.h $$\lim\limits_{x\rightarrow a} f(x)=c$$ ween es zu jeder (Beliebig kleinen) Schranke $\varepsilon>0$, eine $\delta$-umgebung \[{B_\delta }(a): = \left\{ {x \in \mathbb{R}^n}\mid\left| {x - a} \right| < \delta  \right\}\] gibt, so dass $\left| {f(x) - a} \right| < \varepsilon$ für alle $x\in\Omega\cap B_\delta (a), x\not=a$ gilt
\item $f$ heisst in $a\in\Omega$ stetig, wenn $\mathop {\lim }\limits_{x \to a} f'(x) = f(a)$ gilt.
\item $f$ heisst in $\Omega$ stetig, wenn $f$ in allen $a\in\Omega$ stetig ist. 
\end{enumerate}
Die Summe, das Produkt, der Quotient (Nenner ungleich Null) stetiger Funktion sind stetig.\\

$f$ besitzt keinen Grenzwert in $x_0$ wenn sich bei Annäherungen an $x_0$ auf verschiedenen Kurven (z.b. Geraden) verschiedene oder keine Grenzwert ergeben.
\end{definition}

\subsection*{Sandwichlemma}
Sei $f,g,h$ funktionen wobei $g<f<h$. Wenn $\mathop {\lim }\limits_{x \to a} g = L = \mathop {\lim }\limits_{x \to a} h$ gilt, dann ergibt $\lim\limits_{x\rightarrow a}f=L$.\\

\noindent Da $\mathop {\lim }\limits_{(x,y) \to (0,0)} \left| y \right| = 0$ gilt, $\mathop {\lim }\limits_{(x,y) \to (0,0)} f(x,y) = 0 \Rightarrow f$ ist in (0,0) stetig.\\

\noindent \textbf{\underline{Oder}}\\

\noindent Für Grenzwertbestimmungen (also auch für Stetigkeitsuntersuchungen) ist es oft nützlich, die Funktionen mittels Polarkoordinaten umzuschreiben. Vor allem bei Rationalen Funktionen. \\

Hierbei gilt $x=r\cos\theta$, $y=r\sin\theta$, wobei $r=$ länge des Vektors $(x,y)$ und $\varphi$ der Winkel. Nun lass wir die Länge $r$ gegen 0 gehen. 

\subsubsection*{Beispiel}
\begin{enumerate}
\item Die Funktionen 
\begin{itemize}
\item $f(x,y)=x^2+y^2$
\item $f(x,y,z)=x^3+\frac{x^2}{y^2+1}+z$
\item $f(x,y)=4x^2 y^3+3xy$
\item $f(x,y)=\cos xy$
\end{itemize}
sind stetig, da sie aus Steigen Funktionen zusammengesetzt.

\item \[f(x,y) = \left\{ {\begin{array}{*{20}{c}}
{\frac{{{x^2}y}}{{{x^2} + {y^2}}}}&{{\text{für}}}&{{\text{(x,y)}}\not  = (0,0)}\\
0&{{\text{für}}}&{{\text{(x,y)}}= (0,0)}
\end{array}} \right.\]
Für $(x,y)\not=(0,0)$ ist $f$ als Quotient von steiger Funktionen stetig. Es verbleibt $f$ im Punkt $(0,0)$ zu untersuchen. Da \[\left| {\frac{{{x^2}}}{{{x^2} + {y^2}}}} \right| \le 1\] $$ 0<\left| f(x,y)\right| <\left| y\right|$$ \[f(x,y) = \frac{{{x^2}y}}{{{x^2} + {y^2}}} = \frac{{\left( {{r^2}{{\cos }^2}\theta } \right)\left( {r\sin \theta } \right)}}{{{r^2}\left( {{{\cos }^2}\theta  + {{\sin }^2}\theta } \right)}} = r{\cos ^2}\theta \sin \theta \] 
\[\mathop {\lim }\limits_{r \to 0} f(r,\theta ) = \mathop {\lim }\limits_{r \to 0} r{\cos ^2}\theta \sin \theta  = 0\]
\item Wir können nochmals die Stetigkeit der Funktion 
\[f(x,y) = \left\{ {\begin{array}{*{20}{c}}
{\frac{{{x^2}y}}{{{x^2} + {y^2}}}}&{{\text{für}}}&{{\text{(x,y)}}\not  = (0,0)}\\
0&{{\text{für}}}&{{\text{(x,y)}}= (0,0)}
\end{array}} \right.\] mittels Polarkoordinaten untersuchen $$f(x,y)=\frac{r^2\cos\theta\sin\theta}{r^2}=\cos\theta\sin\theta$$ \[\mathop {\lim }\limits_{r \to 0} f(x,y) = \cos \theta \sin \theta \] hängt von $\theta$ ab. $$\Rightarrow f\text{ in (0,0) nicht stetig}$$

\subsubsection*{Bemerkung}\todo{is this supposed to be inside the list or out??}
Eine trickreiche Variante Grenzwerte zu berechnen, ergibt sich durch substitution, d.h. man berechnet den Grenzwert \[\mathop {\lim }\limits_{(x,y) \to ({x_0},{y_0})} f\left( {g(x,y)} \right)\] indem man zunächst $t=g(x,y)$ setzt und den Grenzwert \[{t_0} = \mathop {\lim }\limits_{(x,y) \to ({x_0},{y_0})} g(x,y)\] bestimmt. Dann ist \[\mathop {\lim }\limits_{(x,y) \to ({x_0},{y_0})} f\left( {g(x,y)} \right) = \mathop {\lim }\limits_{t \to {t_0}} f(t)\] 
\end{enumerate}

\subsubsection*{Beispiel}
\[\mathop {\lim }\limits_{(x,y) \to (4,0)} \frac{{\sin xy}}{{xy}}\] Hier ist $g(x,y)=xy$, $\mathop {\lim }\limits_{(x,y) \to (4,0)} g(x,y) = 0$. Somit \[\mathop {\lim }\limits_{(x,y) \to (4,0)} \frac{{\sin xy}}{{xy}} = \mathop {\lim }\limits_{t \to 0} \frac{{\sin t}}{t} = 1\] Wir werden auch sehen das die Existenz der Ableitungen in einigen Richtungen ungenügend für die Differenzierbarkeit der Funktion ist. \\

\noindent\textbf{\underline{Was bedeutet die Ableitung in einiger Richtung?}}
\subsubsection*{Beispiel}
Sei $$f:\mathbb{R}^2\rightarrow \mathbb{R}$$$$(x,y)\rightarrow \left(x^2+xy\right)\cos(xy)$$Man kann für jedes $y$, die Funktion $$\mathbb{R}\rightarrow\mathbb{R}$$$$x\rightarrow \left( x^2+xy\right)\left(\cos xy\right)$$als Funktion einer Variablen $x$ auflassen und die Ableitung davon berechnen. Das Resultat mit $\frac{\partial f}{\partial x}$ bezeichnet, ist die erste partielle Ableitung von $f$ nach $x$. In diesem fall ist es durch \[\frac{{\partial f}}{{\partial x}}(x,y) = (2x + y)(\cos xy) - ({x^2} + xy)y\sin (xy)\] gegeben. \\

\noindent Analog definiert man $\frac{\partial f}{\partial y}$\[\frac{{\partial f}}{{\partial y}}(x,y) = x(\cos xy) - ({x^2} + xy)x\sin (xy)\] Die allgemeine Definition nimmt folgende Gestallt ein. Sei $\Omega \subset\mathbb{R}^n$. In zukunft  bezeichnen wir die $i-$te Koordinate eines Vektors $x\in\mathbb{R}^n$ mit $x^i$; also ist $x=\left( x^1,x^2,\dots,x^n\right)$.\\

\noindent Sei $e_i:=\left( 0,\dots,0,1,0,\dots,0\right)$ der $i-$te Basisvektor von $\mathbb{R}^n$

\begin{definition}{8.2}
Die Funktion $f:\Omega\subset\mathbb{R}^n\rightarrow\mathbb{R}$ heisst an der stelle $x_0\in\Omega$ in Richtung $e_i$ (oder nach $x^i$) partielle differenzierbar falls der limes \[\frac{{\partial f}}{{\partial {x^i}}}({x_0}) = {f_{{x^i}}}({x_0}): =  - \mathop {\lim }\limits_{\begin{array}{*{20}{c}}
{h \to 0}\\
{h\not  = 0}
\end{array}} \frac{{f({x_0} + h{e_i}) - f({x_0})}}{h}\]
\[ = \mathop {\lim }\limits_{\begin{array}{*{20}{c}}
{h \to 0}\\
{h\not  = 0}
\end{array}} \frac{{f\left( {{x_0}^1,{x_0}^2, \ldots ,{x_0}^i + h,{x_0}^{i + 1}, \ldots ,{x_0}^n} \right) - f\left( {{x_0}^1, \ldots ,{x_0}^n} \right)}}{h}\]
existiert
\end{definition}
\subsubsection*{Bemerkung 8.3}
\missingfigure{page 121, middle}
Sei $f:\mathbb{R}^2\rightarrow\mathbb{R}, \left(x_0^1,x_0^2\right)\in\mathbb{R}^2$. Wir betrachten die scharen von $f$ $$f(\cdot ,x_0^2):\mathbb{R}\rightarrow \mathbb{R}$$ und $$f(x_0^1,\cdot ):\mathbb{R}\rightarrow\mathbb{R}$$ $\frac{\partial f}{\partial x^1}$, $\frac{\partial f}{\partial x^2}$ sind die Ansteig der Tangente zur entsprechende schrittkurven

\subsubsection*{Beispiel}
\begin{enumerate}
\item $f(x,y,z)=\cos yz+\sin xy$
\begin{itemize}
\item $\frac{\partial f}{\partial x}=y\cos xy$
\item $\frac{\partial f}{\partial y}=-\sin(yz)\cdot z+\cos(xy)\cdot x$
\item $\frac{\partial f}{\partial z}=-\sin(yz)\cdot y$
\end{itemize}
\item \[f(x,y) = \left\{ {\begin{array}{*{20}{c}}
{\frac{{{x^3}y}}{{{x^2} + {y^2}}}}&{(x,y)\not  = (0,0)}\\
0&{(x,y)\not  = (0,0)}
\end{array}} \right.\]
$$\frac{{\partial f}}{{\partial x}}(0,0) = \mathop {\lim }\limits_{h \to 0} \frac{{f(h,0) - f(0,0)}}{h} = \lim \frac{{\frac{{{h^3} \cdot 0}}{{{h^2}}} - 0}}{h} = 0$$
\[\frac{{\partial f}}{{\partial y}}(0,0) = \mathop {\lim }\limits_{h \to 0} \frac{{f(0,h) - f(0,0)}}{h} = \lim \frac{{\frac{{h \cdot {0^3}}}{{0 + {h^2}}} - 0}}{h} = 0\]
\end{enumerate}
\subsubsection*{Bemerkung}
Für Funktionen $f:\mathbb{R}\rightarrow\mathbb{R}$ einer variable impliziert die differenzierbarkeit in $x_0$, die Stetigkeit in $x_0$ und zudem eine gute Approximation von $f$ durch eine affine Funktion in einer Umgebung von $x_0$. Folgendes Beispiel zeigt, dass in $\mathbb{R}^n$ $(n\geq 2)$ Partielle Differenzierbarkeit keine analoges  Approximationseigenschaften oder stetigkeit impliziert:
\[f: \mathbb{R}^2 \to\mathbb{R} ,{\text{ }}f(x,y) = \left\{ {\begin{array}{*{20}{c}}
{\frac{{xy}}{{{x^2} + {y^2}}}}&{(x,y)\not  = (0,0)}\\
0&{(x,y)\not  = (0,0)}
\end{array}} \right.\]
Für alle $\left( x_0,y_0\right) \in\mathbb{R}^2$ ist $f$ in beiden Richtungen partiel differenzierbar:
\begin{itemize}
\item Für $\left( x_0,y_0\right)\not=\left( 0,0\right)$ \[\frac{{\partial f}}{{\partial x}}\left( {{x_0},{y_0}} \right) = {\left. {\frac{{y\left( {{x^2} + {y^2}} \right) - 2{x^2}y}}{{{{\left( {{x^2} + {y^2}} \right)}^2}}}} \right|_{(x,y) = \left( {{x_0},{y_0}} \right)}} = \frac{{y_0^3 - x_0^2{y_0}}}{{{{\left( {{x_0}^2 + {y_0}^2} \right)}^2}}}\]
\[\frac{{\partial f}}{{\partial y}}\left( {{x_0},{y_0}} \right) = {\left. {\frac{{x\left( {{x^2} + {y^2}} \right) - 2x{y^2}}}{{{{\left( {{x^2} + {y^2}} \right)}^2}}}} \right|_{(x,y)\not  = \left( {{x_0},{y_0}} \right)}} = \frac{{{x^2} - x{y^2}}}{{{{\left( {{x^2} + {y^2}} \right)}^2}}}\]
\item Für $\left(x_0,y_0\right)=\left( 0,0\right)$\[\frac{{\partial f}}{{\partial x}}(0,0) = \mathop {\lim }\limits_{h \to 0} \frac{{\overbrace {f\left( {0 + h,0} \right) - f\left( {0,0} \right)}^{f({x_0} + h{e_1}) - f(x_0)}}}{h} = \lim \frac{0}{h} = 0\]\[\frac{{\partial f}}{{\partial y}}(0,0) = \mathop {\lim }\limits_{h \to 0} \frac{{\overbrace {f\left( {0,0 + h} \right) - f\left( {0,0} \right)}^{f({x_0} + h{e_2}) - f({x_0})}}}{h} = \lim \frac{0}{h} = 0\]
\end{itemize}
Im Ursprung besitzt $f$ beide partielle Ableitungen, sie ist aber nicht stetig. Der Grund ist, dass die partielle Ableitungen nur partielle Informationen geben. Wir müssen die Differenzierbarkeit irgend eine andere weise verallgemeinen.\\

Die Lösung dieses Problem ist, dass man eine Approximations-Eigenschaft durch eine Lineare Abbildung postuliert. \\

Sei $f:\mathbb{R}\rightarrow\mathbb{R}$ differenzierbar in $x_0;f'(x_0)$ existiert. In diesem Fall kann $f$ für alle $x$ nähe $x_0$ durch die Funktion $f(x_0)+f'(x_0)(x-x_0)$ gut approximiert werden. Dass heisst dass $$f(x)=f(x_0)+f'(x_0)(x-x_0)+R(x,x_0)\text{ \hspace{2mm}mit  }\mathop {\lim }\limits_{x \to {x_0}} \frac{{R(x,{x_0})}}{{x - {x_0}}} = 0$$

\subsubsection*{Bemerkung}
$f'(x):\mathbb{R}\rightarrow\mathbb{R}'$ sollt als lineare Abbildung interpretiert werden
\subsection*{Lineare Abbildungen}
Eine Abbildung $A:\mathbb{R}^n\rightarrow\mathbb{R}$ ist linear falls für alle $x,y\in\mathbb{R}^n$ und $\alpha,\beta\in\mathbb{R}$ $$A\left( \alpha x+\beta y\right) =\alpha A(x)+\beta A(y)$$
Eine solche Abbildung ist durch ihre Werte $$A(e_i):=A_1,A(e_2):=A_2,\dots,A(e_n):=A_n$$ auf der Standardbasis $e_1,\dots,e_n$ eindeutig bestimmt. Aus $x = \sum\limits_{i = 1}^n {{x^i}{e_i}} $ und linearität folgt nämlich

\[
A(x) = \sum\limits_{i = 1}^n {{x^i}A({e_i}) = \sum\limits_{i = 1}^n {{A_i}{x^i}} } \tag{\textasteriskcentered}
\]
Umgekehrt bestimmt ein Vektor $\left( A_1,\dots,A_n\right)$ vermöge der Formel (\textasteriskcentered) eine Lineare Abbildung.\\

Schreiben wir $x = \left( {\begin{array}{*{20}{c}}
{{x^1}}\\
 \vdots \\
{{x^n}}
\end{array}} \right)$ für einen Vektor $x = {({x^1})_{1 \le i \le n}}$ und \\ %break needed to have everything on the same line
$A=\left( A_1,\dots,A_n\right)$ für die Darstellung einer Lineare Abbildung $A:\mathbb{R}^n\rightarrow\mathbb{R}$ bezüglich die Standard Basis $\left\{e_1,\dots,e_n\right\}$ so ist \[A(x) = \left( {{A_1}, \ldots ,{A_n}} \right)\left( {\begin{array}{*{20}{c}}
{{x^1}}\\
 \vdots \\
{{x^n}}
\end{array}} \right) = \sum {{A_i}{x^i}} \]

\begin{definition}{8.4}
Die Funktion $f:\Omega\rightarrow\mathbb{R}$ heisst an der Stelle $x_0\in\Omega\subset\mathbb{R}^n$ differenzierbar falls eine lineare Abbildung $A:\mathbb{R}^n\rightarrow\mathbb{R}$ gibt so dass \[f(x) = f\left( {{x_0}} \right) + A\left( {x - {x_0}} \right) + R\left( {{x_0},x} \right)\] wobei $\mathop {\lim }\limits_{x \to {x_0}} \frac{{R\left( {x,{x_0}} \right)}}{{\left| {x - {x_0}} \right|}} = 0$
\end{definition}
In diesem fall heisst $A$ der Differential an der Stelle $x_0$ und wird mit $\mathop {df}\limits_{{x_0}} $ bezeichnet, d.h. $f$ ist total differenzierbar in $x_0=\left( x_0^1,\dots,x_0^n\right)$ falls reelle Zahlen $A_1,\dots,A_n$ existieren so dass gilt \[f(x) = f\left( {{x_0}} \right) + {A_1}\left( {{x^1} - x_0^1} \right) + {A_2}\left( {{x^2} - x_0^2} \right) +  \ldots  + {A_n}\left( {{x^n} - x_0^n} \right) + R\left( {x,{x_0}} \right)\] mit $\mathop {\lim }\limits_{x \to {x_0}} \frac{{R\left( {x,{x_0}} \right)}}{{\left| {x - {x_0}} \right|}} = 0$

\subsubsection*{Bemerkung: Geometrische Interpretation}
Sei $f:\Omega \rightarrow\mathbb{R}$, $\Omega\in\mathbb{R}^2$. Wir können die differenzierbare Funktion nähe dem Punkt $x_0=\left( x_0^1,x_0^2\right)$ mit hilfe der Lineare Funktion \[P\left( x \right) = P\left( {{x^1},{x^2}} \right) = f\left( {x_0^1,x_0^2} \right) + \underbrace {{A_1}\left( {{x^1} - x_0^1} \right) + {A_2}\left( {{x^2} - x_0^2} \right)}_{{d_x}_{_0}f\left( {x - {x_0}} \right)}\] approximieren. \\

Die Differenz $\underbrace {f(x) - P(x)}_{{d_x}_{_0}f\left( {x - {x_0}} \right)}\mathop  \to \limits_{x \to {x_0}} 0$\todo{can't understand what comes after the formula, page 126.1 middle} $P(x)$ ist eine Ebene. Die ist die Tangenteebene zur $f$ an der Stelle $x_0$ und spielt die Rolle des Tangente für Funktionen in einer Variable. \missingfigure{page 126.1 bottom}

\subsubsection*{Beispiel 8.5}
\begin{enumerate}[\indent a)]
\item Jede affin Lineare Funktion $f(x)=Ax+b$, $x\in\mathbb{R}^n$, wobei $a:\mathbb{R}^n\rightarrow\mathbb{R}$ linear, b$\in\mathbb{R}$ ist an jeder stelle $x_0\in\mathbb{R}^n$ differenzierbar, mit $\mathop {df}\limits_{{x_0}} =A$ unabhängig von $x_0$ da 
\[f\left( x \right) - f\left( {{x_0}} \right) - A\left( {x - {x_0}} \right) = 0\hspace{10mm}\forall x,{x_0} \in\mathbb{R}^n\]
\item Koordinaten funktionen $x^i:\mathbb{R}^n\rightarrow\mathbb{R}$, $\left( x^1,x^2,\dots,x^n\right)\rightarrow x^i$, $x^i(x)=x^i$. Dann ist $x^i$ differenzierbar an jeder Stelle $x_0\in\mathbb{R}^n$ mit $${\left. {d{x^i}} \right|_{x = {x_0}}} = \left( {0, \ldots ,0,1,0, \ldots ,0} \right)$$ die Differenziale $dx^1,dx^2,\dots,dx^n$ bilden also an jeder Stelle $x_0\in\mathbb{R}^n$ eine Basis des Raumes $L\left( {\mathbb{R}^n:\mathbb{R}} \right):=\left\{ A:\mathbb{R}^n\rightarrow\mathbb{R}; A\text{ linear}\right\}$, wobei wir $A\in L\left( \mathbb{R}^n:\mathbb{R}\right)$ mit der darstellung $A=\left( A_1,\dots,A_n\right)$ bzg. der Standardbasis $\{ e_1,\dots,e_n\}$ der $\mathbb{R}^n$ identifizieren, und mit $A_i=A\left( e_i\right)$ \[d{x^i} = \left( {0, \ldots ,0,1,0, \ldots ,0} \right)\]\[\left( {d{x^i}\left( {{e_1}} \right),d{x^i}\left( {{e_2}} \right), \ldots ,d{x^i}\left( {{e_n}} \right)} \right)\]
Da gilt $d{x^i}\left( {{e_j}} \right) = \left\{ {\begin{array}{*{20}{c}}
1&{i = j}\\
0&{i\not  = j}
\end{array}} \right.$ ist ${\left( {d{x^i}} \right)_{1 \le i \le n}}$ die duale Basis von $L\left( \mathbb{R}^n:\mathbb{R}\right)$ zur Standardbasis ${\left( {e_i} \right)_{1 \le i \le n}}$ des $\mathbb{R}^n$.
\item Jedes $f:\mathbb{R}\rightarrow\mathbb{R}\in\subset '\left(\mathbb{R}\right)$ besitzt das Differential \[df\left( {{x_0}} \right) = \frac{{df}}{{dx}}\left( {{x_0}} \right)dx = f'\left( {{x_0}} \right)dx\] d.h. $f'\left(x_0\right)$ ist die Darstellung von $df\left(x_0\right)$ bezüglich der Basis $dx$ von $L\left(\mathbb{R}:\mathbb{R}\right)$ 
\item $f\left(x,y\right)=xe^y$, $\mathbb{R}^2\rightarrow\mathbb{R}$ ist an jeder Stelle $\left(x_0,y_0\right)\in\mathbb{R}^2$ differenzierbar und es gilt \[df\left( {{x_0},{y_0}} \right) = \left( {\frac{{\partial f}}{{\partial x}}\left( {{x_0},{y_0}} \right),\frac{{\partial f}}{{\partial y}}\left( {{x_0},{y_0}} \right)} \right) = \left( {{e^{{y_0}}},x{e^{{y_0}}}} \right)\] \[f\left( {x,y} \right) - f\left( {{x_0},{y_0}} \right) = \underbrace {f\left( {x,y} \right) - f\left( {{x_0},y} \right)}_ \swarrow  + f\left( {{x_0},y} \right) - f\left( {{x_0},{y_0}} \right)\] \[ = \frac{{\partial f}}{{\partial x}}\left( {\xi ,y} \right)\left( {x - {x_0}} \right) + \frac{{\partial f}}{{\partial y}}\left( {{x_0},\eta } \right)\left( {y - {y_0}} \right)\]
Nach der MWS der DR, mit geeigneten Zwischenstellen $\xi=\xi (y)$ und $\eta$ \[ = \frac{{\partial f}}{{\partial x}}\left( {{x_0},{y_0}} \right)\left( {x - {x_0}} \right) + \frac{{\partial f}}{{\partial y}}\left( {{x_0},{y_0}} \right)\left( {y - {y_0}} \right) + R\left( {x,y} \right)\] 
mit
\begin{align*}
 R\left( {x,y} \right) = &\left[ {\frac{{\partial f}}{{\partial x}}\left( {\xi ,y} \right) - \frac{{\partial f}}{{\partial x}}\left( {{x_0},{y_0}} \right)} \right]\left( {x - {x_0}} \right) \\
 + &\left[ {\frac{{\partial f}}{{\partial y}}\left( {{x_0},\eta } \right) - \frac{{\partial f}}{{\partial y}}\left( {{x_0},{y_0}} \right)} \right]\left( {y - {y_0}} \right)
\end{align*}
Wegen die Stetigkeit der Funktionen \[\frac{{\partial f}}{{\partial x}}\left( {x,y} \right) = {e^y}\hspace{5mm}\text{und}\hspace{5mm}\frac{{\partial f}}{{\partial y}}\left( {x,y} \right) = x{e^y}\] können wir den ``Fehler'' $R\left( x,y\right)$ leicht abschätzen \[\frac{{\left| {R\left( {x,y} \right)} \right|}}{{\left| {\left( {x,y} \right) - \left( {{x_0},{y_0}} \right)} \right|}} \le \mathop {\sup }\limits_{\begin{array}{*{20}{c}}
{\left| {\xi  - {x_0}} \right| < \left| {x - {x_0}} \right|}\\
{\left| {\eta  - {y_0}} \right| < \left| {y - {y_0}} \right|}
\end{array}} \left( {\left| {{e^y} - {e^{{y_0}}}} \right| + \left| {{x_0}} \right|\left| {{e^\eta } - {e^{{y_0}}}} \right|} \right)\]
Für $\left( x,y\right)\rightarrow\left( x_0,y_0\right)$, $\left( x,y\right)\not=\left( x_0,y_0\right)$: d.h. es gilt \[\frac{{R\left( {x,y} \right)}}{{\left| {\left( {x,y} \right) - \left( {{x_0},{y_0}} \right)} \right|}} \to 0\] 
d.h. es gilt \[\frac{{f\left( {x,y} \right) - f\left( {{x_0},{y_0}} \right) - \frac{{\partial f}}{{\partial x}}\left( {{x_0},{y_0}} \right)\left( {x - {x_0}} \right) - \frac{{\partial f}}{{\partial y}}\left( {{x_0},{y_0}} \right)\left( {y - {y_0}} \right)}}{{\left| {\left( {x,y} \right) - \left( {{x_0},{y_0}} \right)} \right|}}\mathop  \to \limits_{\left( {x,y} \right) \to \left( {{x_0},{y_0}} \right)} 0\]
 d.h. $f\left( x,y\right)$ ist \todo{can't read, page 130 bottom} differenzierbar und 
\[df\left( {{x_0},{y_0}} \right) = \left( {\frac{{\partial f}}{{\partial x}}\left( {{x_0},{y_0}} \right),\frac{{\partial f}}{{\partial y}}\left( {{x_0},{y_0}} \right)} \right)\]
\item Die Funktion \[f\left( {x,y} \right) = \left\{ {\begin{array}{*{20}{c}}
{\frac{{{x^3}y}}{{{x^2} + {y^2}}}}&{\left( {x,y} \right)\not  = \left( {0,0} \right)}\\
0&{\left( {x,y} \right) = \left( {0,0} \right)}
\end{array}} \right.\] ist in $\left( 0,0\right)$ differenzierbar. \\

Wir haben schon gesehen dass $\frac{\partial f}{\partial x}\left( 0,0\right)=0$ und $\frac{\partial f}{\partial y}\left( 0,0\right)=0$. Dann gilt \[\frac{{\left| R \right|}}{{\left| {\left( {x,y} \right)} \right|}} = \frac{{\left| {f\left( {x,y} \right) - f\left( {0,0} \right) - \frac{{\partial f}}{{\partial x}}\left( {0,0} \right)\left( {x - 0} \right) - \frac{{\partial f}}{{\partial y}}\left( {0,0} \right)\left( {y - 0} \right)} \right|}}{{\left| {\left( {x - 0,y - 0} \right)} \right|}}\] \[ = \frac{{\left| {f\left( {x,y} \right) - 0 - 0 - 0} \right|}}{{\left| {\left( {x,y} \right)} \right|}} = \frac{{\left| {f\left( {x,y} \right)} \right|}}{{\left| {\left( {x,y} \right)} \right|}}\] 
Zum untersuchen ist 
\[\mathop {\lim }\limits_{\left( {x,y} \right) \to \left( {0,0} \right)}  \frac{{\left| {R\left( {\left( {x,y} \right),\left( {0,0} \right)} \right)} \right|}}{{\left( {x,y} \right) - \left( {0,0} \right)}} = \mathop {\lim }\limits_{\left( {x,y} \right) \to \left( {0,0} \right)} \frac{{\left| {f\left( {x,y} \right)} \right|}}{{\left| {\left( {x,y} \right)} \right|}}\]
Mittels Polarkoordinaten ist dies noch einsichtiger 
\[\mathop {\lim }\limits_{\left( {x,y} \right) \to \left( {0,0} \right)} \frac{{\left| {f\left( {x,y} \right)} \right|}}{{{x^2} + {y^2}}} = \mathop {\lim }\limits_{r \to 0} \frac{{{r^4}{{\cos }^3}\theta \sin \theta }}{{{r^2}}} = \mathop {\lim }\limits_{r \to 0} {r^2}{\cos ^3}\theta \sin \theta  = 0\] $$\Rightarrow f\text{ in }\left( 0,0\right)\text{ differenzierbar}$$
\end{enumerate}

\noindent Gibt es eine Beziehung zwischen des Differential und der partielle Ableitungen?

\subsubsection*{Bemerkung 8.6}

Sei $f:\Omega\rightarrow\mathbb{R}$, $\Omega\subset\mathbb{R}^n$ differenzierbar an der Stelle $x_0\in\Omega$. Dann existieren die partiellen Ableitungen $\frac{\partial f}{\partial x^i}\left( x_0\right)$, $i=1,\dots,n$ und dass Differential kann \[{d_{{y_0}}}f = \left( {\frac{{\partial f}}{{\partial {x^1}}}\left( {{x_0}} \right), \ldots ,\frac{{\partial f}}{{\partial {x^n}}}\left( {{x_0}} \right)} \right)\] dargestellt werden. 

\subsubsection*{Beweis}
$f$ an der Stelle $x_0$ differenzierbar \[\Rightarrow f\left( {{x_0} + h{e_i}} \right) = f\left( {{x_0}} \right) + \left( {{d_{{x_0}}}f} \right)\left( {h{e_i}} \right) + R\left( {{x_0} + h{e_i},{x_0}} \right)\] wobei \[\mathop {\lim }\limits_{h \to 0} \frac{{R\left( {{x_0} + h{e_i},{x_0}} \right)}}{h} = \lim \frac{{f\left( {{x_0} + h{e_i}} \right) - f\left( {{x_0}} \right)\left( {{d_{{x_0}}}f\left( {h{e_i}} \right)} \right)}}{h} = 0\] \[ \Rightarrow \lim \frac{{f\left( {{x_0} + h{e_i}} \right) - f\left( {{x_0}} \right)}}{h} = \lim \frac{{h{d_{{x_0}}}f\left( {{e_i}} \right)}}{h} = {d_{{x_0}}}f\left( {{e_i}} \right)\] d.h. $\frac{\partial f}{\partial x^i}\left( x_0\right)$ existiert und $=d_{x_0}f\left( e_i\right)$. \\

Da $\left( dx^i\right)_{i=1,\dots,n}$ die zur $\left( e_j\right)_{1\leq j\leq n}$ duale Basis ist \[{d_{{x_0}}}f = \sum\limits_{i = 1}^n {\frac{{\partial f}}{{\partial {x^i}}}\left( {{x_0}} \right)d{x^i} = \left( {\frac{{\partial f}}{{\partial {x^1}}}\left( {{x_0}} \right),\frac{{\partial f}}{{\partial {x^2}}}\left( {{x_0}} \right), \ldots ,\frac{{\partial f}}{{\partial {x^n}}}\left( {{x_0}} \right)} \right)} \]

\subsubsection*{Beispiel}
Die Funktion \[f\left( {x,y} \right) = \left\{ {\begin{array}{*{20}{c}}
{\frac{{xy}}{{{x^2} + {y^2}}}}&{\left( {x,y} \right)\not  = \left( {0,0} \right)}\\
0&{\left( {x,y} \right) = \left( {0,0} \right)}
\end{array}} \right.\] ist in $\left( 0,0\right)$ nicht differenzierbar ($f$ ist  in $\left( 0,0\right)$ nicht stetig)

\subsubsection*{Satz 8.7}
Falls $f:\Omega\rightarrow\mathbb{R}$ in $x_0\in\Omega\subset\mathbb{R}$ differenzierbar, ist sie in $x_0$ auch stetig.

\subsubsection*{Beweis}
Folgt aus der Definition
\begin{definition}{8.8}
$f:\Omega\rightarrow\mathbb{R}$ heisst von der Klasse $C'$, $\left( f\in C'\left(\Omega\right)\right)$ falls $f$ an jeder Stelle $x_0\in\Omega$ und in jede Richtung $e_i$ partielle differenzierbar ist und die Funktionen $x\rightarrow\frac{\partial f}{\partial x^i}\left(x\right)$ für jedes $1\leq i\leq n$ auf $\Omega$ stetig sind
\end{definition}
\subsubsection*{Satz 8.9}
Sei $f\in C'\left(\Omega\right)$. Dann ist $f$ an jeder Stelle $x_0\in\Omega$ differenzierbar. 

\subsubsection*{Beweis}
Für $n=3$ seien $x=\left( x^1,x^2,x^3\right)$, $x_0=\left( x_0^1,x_0^2,x_0^3\right)$. Dann ist 
\begin{align*}
f\left( x \right) - f\left( {{x_0}} \right) = &\left\{ {f\left( {{x^1},{x^2},{x^3}} \right) - f\left( {{x^1},{x^2},x_0^3} \right)} \right\}\\
+ &\left\{ {f\left( {{x^1},{x^2},x_0^3} \right) - f\left( {{x^1},x_0^2,x_0^3} \right)} \right\}\\
+ &\left\{ {f\left( {{x^1},x_0^2,x_0^3} \right) - f\left( {x_0^1,x_0^2,x_0^3} \right)} \right\}
\end{align*}
Nach dem MWS der DR gilt: \[f\left( {{x^1},x_0^2,x_0^3} \right) - f\left( {x_0^1,x_0^2,x_0^3} \right) = \frac{{\partial f}}{{\partial {x^1}}}\left( {{\xi ^1},x_0^2,x_0^3} \right)\left( x^1-x_0^1\right)\] 
wobei $\xi^1$ zwischen $x_0^1$ und $x^1$. Analog: 
\[f\left( {{x^1},{x^2},x_0^3} \right) - f\left( {{x^1},x_0^2,x_0^3} \right) = \frac{{\partial f}}{{\partial {x^2}}}\left( {{x^1},{\xi ^2},x_0^3} \right)\left( {{x^2} - x_0^2} \right)\] wobei $\xi^2\in\left(x_0^2,x^2\right)$ 
und \[f\left( {{x^1},{x^2},{x^3}} \right) - f\left( {{x^1},{x^2},x_0^3} \right) = \frac{{\partial f}}{{\partial {x^3}}}\left( {{x^1},{x^2},{\xi ^3}} \right)\left( {{x^3} - x_0^3} \right)\]
Eingesetz in dem Ausdrucke für $f(x)-f(x_0)$ ergibt: 
\begin{align*}
f\left( x \right) - f\left( {{x_0}} \right) = &\frac{{\partial f}}{{\partial {x^1}}}\left( {{\xi ^1},x_0^2,x_0^3} \right)\left( {{x^1} - x_0^1} \right)\\
+ &\frac{{\partial f}}{{\partial {x^2}}}\left( {{x^1},{\xi ^2},x_0^3} \right)\left( {{x^2} - x_0^2} \right)\\ + &\frac{{\partial f}}{{\partial {x^3}}}\left( {{x^1},{x^2},{\xi ^3}} \right)\left( {{x^3} - x_0^3} \right)
\end{align*}
Also
\[f\left( x \right) - f\left( {{x_0}} \right) = \sum\limits_{i = 1}^3 {\frac{{\partial f}}{{\partial {x^1}}}\left( {x_0^1,x_0^2,x_0^3} \right)\left( {{x^i} - x_0^i} \right)}  + R\left( {{x_0},x} \right)\]
Wobei 
\begin{align*}
R\left( {{x_0},x} \right) = &\left( {\frac{{\partial f}}{{\partial {x^1}}}\left( {{\xi ^1},x_0^2,x_0^3} \right) - \frac{{\partial f}}{{\partial {x^1}}}\left( {x_0^1,x_0^2,x_0^3} \right)} \right)\left( {{x^1} - x_0^1} \right)\\
 + &\left( {\frac{{\partial f}}{{\partial {x^2}}}\left( {{x^1},{\xi ^2},x_0^3} \right) - \frac{{\partial f}}{{\partial {x^2}}}\left( {x_0^1,x_0^2,x_0^3} \right)} \right)\left( {{x^2} - x_0^2} \right)\\
 + &\left( {\frac{{\partial f}}{{\partial {x^3}}}\left( {{x^1},{x^2},{\xi ^3}} \right) - \frac{{\partial f}}{{\partial {x^3}}}\left( {x_0^1,x_0^2,x_0^3} \right)} \right)\left( {{x^3} - x_0^3} \right)
\end{align*}
Also
\[\left| {R\left( {x,{x_0}} \right)} \right| < \left| {x - {x_0}} \right|\underbrace {\left\{ {\left| {\left(  \ldots  \right)} \right| + \left| {\left(  \ldots  \right)} \right| + \left| {\left(  \ldots  \right)} \right|} \right\}}_{\scriptstyle \to 0{\text{ mit }}x \to {x_0}\hfill\atop
\scriptstyle{\text{weil }}\frac{{\partial f}}{{\partial {x^i}}}{\text{ stetig sind}}\hfill}\]
Also $\lim\frac{R\left( x,x_0\right)}{\left( x-x_0\right)}=0$ und $f(x)$ ist differenzierbar.

\subsubsection*{Beispiel 8.10}
Polynome auf $\mathbb{R}^n$ sind von Klasse $C!$. Für jedes Multindex $\alpha=\left(\alpha_0,\dots , \alpha_n\right)\in\mathbb{N}^n$ definieren wir die Monomialfunktion \[{x^\alpha }: = {\left( {{x^0}} \right)^{{\alpha _0}}}{\left( {{x^1}} \right)^{{\alpha _1}}} \ldots {\left( {{x^n}} \right)^{{\alpha _n}}}\] Ein polynom von Grad $\leq N$ ist dann gegeben durch \[p(x) = \sum\limits_{\left| \alpha  \right| \le N} {{a_\alpha }{x^\alpha }} \] wobei $\left| \alpha  \right| = {\alpha _0} +  \ldots  + {\alpha _n}$
\todo[inline]{Pages 135.1 - 135.2 are a zusammenfassung, not sure if needed to be included}
\section{Differentiationsregeln}
Ganz analog zum eindimensionalen Fall gelten folgende Differentiationsregeln

\subsubsection*{Satz 8.11}
Sei $\Omega\subset\mathbb{R}^n$, sowie $f,g:\Omega\rightarrow\mathbb{R}$ an der Stelle $x_0\in\Omega$ differenzierbar. Dann gilt \begin{enumerate}
\item $d\left( {f + g} \right)\left( {{x_0}} \right) = df\left( {{x_0}} \right) + dg\left( {{x_0}} \right)$
\item $d\left( {fg} \right)\left( {{x_0}} \right) = g\left( {{x_0}} \right)df\left( {{x_0}} \right) + f\left( {{x_0}} \right)dg\left( {{x_0}} \right)$
\item Falls $g\left( x_0\right)\not=0$ \[d\left( {\frac{f}{g}} \right)\left( {{x_0}} \right) = \frac{{g\left( {{x_0}} \right)df\left( {{x_0}} \right) - f\left( {{x_0}} \right)dg\left( {{x_0}} \right)}}{{{{\left( {g\left( {{x_0}} \right)} \right)}^2}}}\]
\end{enumerate}
Beweis ist der selbe wie in Dim=1. Für die Kettenregel gibt es mehrere variationen 

\subsubsection*{Satz 8.12 (Kettenregel, 1. Version)}
Sei $g:\Omega\rightarrow\mathbb{R}$ in $x_0\in\Omega\subset\mathbb{R}^n$ differenzierbar, sowie $f\mathbb{R}\rightarrow\mathbb{R}$ an der stelle $g\left( x_0\right)\in\mathbb{R}$ differenzierbar. Dann gilt \[d\left( {f \circ g} \right)\left( {{x_0}} \right) = f'\left( {g\left( {{x_0}} \right)} \right) \cdot dg\left( {{x_0}} \right)\]

\missingfigure{page 137, middle}

\subsubsection*{Beweis}
$g$ an der Stelle $x_0$ differenzierbar 
\[ \Rightarrow g\left( x \right) - g\left( {{x_0}} \right)\mathop  = \limits^A dg\left( {{x_0}} \right)\left( {x - {x_0}} \right) + {R_g}\left( {x - {x_0}} \right)\]
mit \[\frac{{{R_g}\left( {x - {x_0}} \right)}}{{\left( {x - {x_0}} \right)}}\mathop  \to \limits_{x \to {x_0}} 0 \Rightarrow \frac{{g\left( x \right) - g\left( {{x_0}} \right)}}{{\left| {x - {x_0}} \right|}}\mathop  \le \limits^B C = \max \left[ {\frac{{\partial g}}{{\partial {x^i}}}\left( {{x_0}} \right)} \right]\] $f$ in $g\left( x_0\right)$ differenzierbar 
\[f\left( {g\left( x \right)} \right) - f\left( {g\left( {{x_0}} \right)} \right)\mathop  = \limits^C f'\left( {g\left( {{x_0}} \right)} \right)\left[ {g(x) - g\left( {{x_0}} \right)} \right] + {R_f}\left( {g\left( x \right),g\left( {{x_0}} \right)} \right)\]
Woraus folgt:
\begin{align*}
f\left( {g\left( x \right)} \right) - f\left( {g\left( {{x_0}} \right)} \right) = & f'\left( {g\left( {{x_0}} \right)} \right)\left[ {dg\left( {{x_0}} \right)\left( {x - {x_0}} \right) + {R_g}\left( {x,{x_0}} \right)} \right]\\ + & {R_f}\left( {g\left( {{x_0}} \right),g\left( x \right)} \right)
\end{align*}

Aus B folgt:
\[\frac{{{R_f}\left( {g\left( {{x_0}} \right),g\left( x \right)} \right)}}{{x - {x_0}}} = \underbrace {\underbrace {\frac{{{R_f}\left( {g\left( {{x_0}} \right) - g\left( x \right)} \right)}}{{\left| {g\left( x \right) - g\left( {{x_0}} \right)} \right|}}}_{\begin{array}{*{20}{c}}
 \downarrow \\
0
\end{array}} \cdot \underbrace {\frac{{\left| {g\left( x \right) - g\left( {{x_0}} \right)} \right|}}{{\left| {x - {x_0}} \right|}}}_{\mathop  \le \limits^B C}}_{\begin{array}{*{20}{c}}
 \downarrow \\
0
\end{array}}\]
d.h.
\[f\left( {g\left( x \right)} \right) - f\left( {g\left( {{x_0}} \right)} \right) = \left( {f'\left( {g\left( {{x_0}} \right)} \right) \cdot dg\left( {{x_0}} \right)} \right)\left( {x - {x_0}} \right) + {R_{f \circ g}}\left( {x,{x_0}} \right)\]
wobei
\[{R_{f \circ g}}\left( {x,{x_0}} \right) = f'\left( {g\left( {{x_0}} \right)} \right){R_g}\left( {x,{x_0}} \right) + {R_f}\left( {g\left( {{x_0}} \right),g\left( x \right)} \right)\]
und 
\[\frac{{{R_{f \circ g}}\left( {x,{x_0}} \right)}}{{x - {x_0}}} = \underbrace {f'\left( {g\left( {{x_0}} \right)} \right)\frac{{{R_g}\left( {x,{x_0}} \right)}}{{\left( {x - {x_0}} \right)}}}_{ \downarrow 0} + \underbrace {\frac{{{R_f}\left( {g\left( {{x_0}} \right),g\left( x \right)} \right)}}{{x - {x_0}}}}_{ \downarrow 0}\]

\subsubsection*{Beispiel 8.13}
Sei $h:\mathbb{R}^2\rightarrow\mathbb{R}$ $$h(x,y)=e^{xy}$$ $h= f\circ g$ wobei $g(x,y)=xy$, $f(t)=e^t$. Dann ist einerseits \[dh\left( {x,y} \right) = \left( {\frac{{\partial h}}{{\partial x}},\frac{{\partial h}}{{\partial y}}} \right) = \left( {y{e^{xy}},x{e^{xy}}} \right)\] anderseits nach Kettenregel 
\[dh\left( {x,y} \right) = d\left( {f \circ g} \right)' = f'\left( {g\left( {x,y} \right)} \right) \cdot dg\left( {x,y} \right) = {e^{xy}} \cdot \left( {y,x} \right) = \left( {y{e^{xy}},x{e^{xy}}} \right)\]
Für die nächste Kettenregel führen wir folgende Definition ein:

\begin{definition}{8.14}
Sei $\Omega \subset\mathbb{R}$ und $f=\left( f_1,\dots,f_n\right):\Omega\rightarrow\mathbb{R}^n$ eine Abbildung. Dann ist $f$ an der Stelle $x_0\in\mathbb{R}$ differenzierbar, falls jede Komponentenfunktion $f_i$ an der Stelle $x_0$ differenzierbar ist. Wir definieren in diesem Fall \[f'\left( {{x_0}} \right): = \left( {{f_1}'\left( {{x_0}} \right),{f_2}'\left( {{x_0}} \right), \ldots ,{f_n}'\left( {{x_0}} \right)} \right)\]
\end{definition}

\subsubsection*{Bemerkung 8.15}
$f'\left( x_0\right)$ kann als Geschwindigkeitsvektor im Punkt $f\left( x_0\right)$ aufgefasst werden.

\subsubsection*{Satz 8.16 (Kettenregel 2. Version)}
Sei $\Omega\subset\mathbb{R}^n$,$I\subset\mathbb{R}$. Sei $g:I\rightarrow\Omega$, $t\rightarrow\left( g_1(t), g_2(t),\dots, g_n(t)\right)$, an der Stelle $t_0\in I$ differenzierbar sowie $f:\Omega\rightarrow\mathbb{R}$ an der Stelle $g\left( t_0\right)$ differenzierbar. Dann gilt:
\[\frac{d}{{dt}}\left( {f \circ g} \right)\left( {{t_0}} \right) = df\left( {g\left( {{t_0}} \right)} \right) \cdot g'\left( {{t_0}} \right)\]
\begin{align*}
d\left( {f \circ g} \right)\left( {{t_0}} \right) = &df\left( {g\left( {{t_0}} \right)} \right) \cdot dg\left( {{t_0}} \right)\\
 = &\frac{{\partial f}}{{\partial {x^1}}}\left( {g\left( {{t_0}} \right)} \right) \cdot \frac{{d{g_1}}}{{dt}}\left( {{t_0}} \right) + \frac{{\partial f}}{{\partial {x^2}}}\left( {g\left( {{t_0}} \right)} \right) \cdot \frac{{d{g_2}}}{{dt}}\left( {{t_0}} \right)\\ & + \ldots  + \frac{{\partial f}}{{\partial {x^n}}}\left( {g\left( {{t_0}} \right)} \right) \cdot \frac{{d{g_n}}}{{dt}}\left( {{t_0}} \right)
\end{align*}
\missingfigure{page 141, middle}

\subsubsection*{Beispiel 8.17}
Die vier Grundrechenarten sind differenzierbare Funktionen von zwei variablen. Insbesondere gilt: 
\begin{itemize}
\item $a:\mathbb{R}^2\rightarrow\mathbb{R}$, $\left( x,y\right) \rightarrow x+y$ \[da\left( {x,y} \right) = \left( {\frac{{\partial a}}{{\partial x}},\frac{{\partial a}}{{\partial y}}} \right) = \left( {1,1} \right)\]
\item $m:\mathbb{R}^2\rightarrow \mathbb{R}$, $\left( x,y\right) \rightarrow x\cdot y$ \[dm\left( {x,y} \right) = \left( {y,x} \right)\]
\end{itemize}
Setzt man diese beiden Funktionen in die obige Kettenregel ein, so erhält man die aus der Analysis I bekannte Summen und Produktregel: 
$$g:\mathbb{R}\rightarrow\mathbb{R}^2, t\rightarrow \left( g_1(t),g_2(t)\right)$$
\[\frac{d}{{dt}}\left( {{g_1} + {g_2}} \right) = \frac{d}{{dt}}\left( {a \circ g} \right) = \left( {1,1} \right) \cdot \left( {\frac{{d{g_1}}}{{dt}},\frac{{d{g_2}}}{{dt}}} \right) = 1\cdot \frac{{d{g_1}}}{{dt}} + 1\cdot \frac{{d{g_2}}}{{dt}}\]
und 
\begin{align*}
\frac{d}{{dt}}\left( {{g_1} \cdot {g_2}} \right) = \frac{d}{{dt}}\left( {m \circ g} \right) = &\left( {\left( {dm} \right)\left( {g(t)} \right)} \right) \cdot \left( {\frac{{dg}}{{dt}}} \right)\\ 
= &\left( {{g_2}\left( t \right),{g_1}\left( t \right)} \right) \cdot \left( {\frac{{d{g_1}}}{{dt}},\frac{{d{g_2}}}{{dt}}} \right)\\ 
= &\frac{{d{g_1}}}{{dt}} \cdot {g_2}\left( t \right) + \frac{{d{g_2}}}{{dt}} \cdot {g_1}\left( t \right)
\end{align*}

\subsubsection*{Beispiel 8.18}
Sei $f:\Omega\rightarrow\mathbb{R}$ differenzierbar an der Stelle $x_0\in\Omega$ und sei $e\in\mathbb{R}^n\backslash\{ 0\}$; mit $\left| e\right|=1$. Betrachte die Gerade $g(t)=x_0+te$, $t\in\mathbb{R}$ durch $x_0$ mit Richtungsvektor 

\missingfigure{page 143, top. ALSO ADD THE FORMULAS ON THE RIGHT PAGE USING A MINIPAGE}

Dann ist die Funktion $f\circ g$ in einer Umgebung von $t_0=0$ definiert und nach Kettenregel $f\circ g$ an der Stelle $t_0=0$ differenzierbar mit 
\[\frac{d}{{dt}}\left( {f \circ g} \right)\left( 0 \right) = df\left( {g\left( 0 \right)} \right)\frac{{dg}}{{dt}}\left( 0 \right) = df\left( {{x_0}} \right)\left( e \right) = \sum\limits_{i = 1}^n {\frac{{\partial f}}{{\partial {x^i}}}\left( {{x_0}} \right) \cdot {e^i}} \]
$e=\left( e^1,\dots, e^n\right) $ und wird Richtungsableitung von $f$ in Richtung $e$ genannt; $\partial_ef\left( x_0\right)$ bezeichnet. Insbesondere gilt für $e=e_i$
\[{\partial _{{e_i}}}f\left( {{x_0}} \right) = \frac{{\partial f}}{{\partial {x^i}}}\left( {{x_0}} \right) = df\left( {{x_0}} \right)\left( {{e_i}} \right)\]
Geometrisch die Richtungsableitung von $f$ in Richtung $e$ ist genau die Steigung der Tangente zur Schnittkurve falls wir den Graph von $f$ mit einer zur Ebene $xy$ senkrecht Ebene durch $x_0+te$ scheiden.
\missingfigure{page 144, middle}
Für den Mittelwertsatz der DR - zu verallgemeinen benützen wir folgenden Begriffen:
\begin{definition}{8.19}
Eine Menge $K\subset \mathbb{R}^n$ ist genau dann Konvex falls für jede Paar von Punkten $x,y\in K$ die Menge $K$ auch das segment $$\left( 1-t\right) x+ty\hspace{10mm} t\in\lbrack 0,y\rbrack$$ mit endpunkt $x,y$ enthält
\missingfigure{Page 145, middle}
\end{definition}
\subsubsection*{Satz 8.20}
Sei $\Omega\subset\mathbb{R}^n$ konvex $f:\Omega\rightarrow\mathbb{R}$ differenzierbar, $x_0,x_1\in\Omega$ sowie $x_t:=\left( 1-t\right) x_0+tx_1$\todo{is it $tx_1$ or $tx$, ?? page 145 middle}. Dann gibt es $\vartheta\in\lbrack 0,1\rbrack$ mit \[f\left( {{x_1}} \right) - f\left( {{x_0}} \right) = df\left( {{x_{i\vartheta }}} \right)\left( {{x_1} - {x_0}} \right)\]

\subsubsection*{Beweis}
Sei $g\left( t\right) = \left( 1-t\right) x_0+tx_1=x_t$. Dann ist $t\rightarrow \left( f\circ g\right)(t)$ auf $\left[ 0,1\right]$ stetig und in $\left( 0,1\right)$ differenzierbar. Also gibt es $\vartheta\in\left( 0,1\right)$ mit (nach MWS der DS einer variable) \[f\left( {{x_i}} \right) - f\left( {{x_0}} \right) = \left( {f \circ g} \right)(1) - \left( {f \circ g} \right)(0) = \left( {f \circ g} \right)'\left( \vartheta  \right)\left( {1 - 0} \right)\] Nur ist \[\left( {f \circ g} \right)'\left( \vartheta  \right) = df\left( {g\left( \vartheta  \right) \cdot \frac{{dg}}{{dt}}\left( \vartheta  \right)} \right)\] \todo{Is the formula done or does it continue on a new line, page 146 top}
%\[\left( {df} \right)\left( {{x_\vartheta }} \right)\left( { - {x_0} + {x_1}} \right)\]
Die Kettenregel wird auch angewandelt um Integrale mit Parametern zu studieren. Ein Beispiel davon ist:

\subsubsection*{Beispiel}
Sei $h:\mathbb{R}^2\rightarrow\mathbb{R}$, $\left( s,t\right)\rightarrow h\left(s,t \right)$. Wir nehmen an, $h$ ist stetig, $\frac{\partial h}{\partial t}$ existiert und ist uf ganz $\mathbb{R}^2$ stetig. Sei 
\[u(t) = \int\limits_a^{b(t)} {h(s,t)ds} {\text{, }}b(t) \in\subset '\left( \mathbb{R} \right){\text{, }}a \in\mathbb{R}\]\todo{Is it $C'$ or $\subset '$?? page 146 middle}

\subsubsection*{Satz 8.21}
Sei $h(s,t)$ eine stetige differenzierbare Funktion von zwei variablen und $b(t)$ differenzierbare Funktion eine variable. Dann ist die Funktion \[u(t): = \int\limits_a^{b(t)} {h(s,t)ds} \] wo definiert, differenzierbar mit der Ableitung \[u'(t): = h\left( {b(t),t} \right) \cdot b'(t) + \int\limits_a^{b(t)} {\frac{{\partial h}}{{\partial t}}\left( {s,t} \right)ds} \]
\subsubsection*{Korollar 8.22}
Sei $h=h\left( s,t\right) :\mathbb{R}^2\rightarrow\mathbb{R}$ stetig, und $\frac{\partial h}{\partial t}$ existiert und auf ganz $\mathbb{R}^2$ stetig. Sei \[u(t) = \int\limits_0^t {h\left( {s,t} \right)ds} \] Dann \[u(t) \in  \subset '\left( \mathbb{R}\right)\text{ und }u'(t) = h\left( {t,t} \right) + \int\limits_0^t {\frac{{\partial h}}{{\partial t}}\left( {s,t} \right)ds} \]

\subsubsection*{Beweis}
Setze $b(t)=t$, $a=0$ in Satz 8.21.

\subsubsection*{Korollar 8.23}
Sei $h:\mathbb{R}^2\rightarrow\mathbb{R}$ eine stetige Funktion mit Stetiger partieller Ableitung $\frac{\partial h}{\partial t}$. Dann ist die Funktion \[u(t): = \int\limits_a^b {h(s,t)ds} \] differenzierbar mit Ableitung \[u'(t): = \int\limits_a^b {\frac{{\partial h}}{{\partial t}}(s,t)ds} \] 

\subsubsection*{Beweis}
Setze $b(t)=b$, in Satz 8.20

\subsubsection*{Bemerkung 8.24}
Mit Korollar 8.23 kann man bestimmte Integrale berechnen, auch wenn die zugehörige unbestimmten Integrale nicht elementar darstellbar sind
\subsubsection*{Beispiel 8.25}
Berechne das integral \[\int\limits_0^1 {\frac{{{x^5} - 1}}{{\log x}}} dx\] Sei \[u\left( \alpha  \right): = \int\limits_0^1 {\frac{{{x^\alpha } - 1}}{{\log x}}dx} \] Für $\alpha \geq 0$ erfüllt $u\left(\alpha\right)$ die Voraussetzungen des Satzes. Wir berechnen \[u'\left( \alpha  \right) = \int\limits_0^1 {\frac{\partial }{{\partial \alpha }}\left( {\frac{{{x^\alpha } - 1}}{{\log x}}} \right)dx}  = \int\limits_0^1 {\frac{{{x^\alpha }\log x}}{{\log x}}dx = \int\limits_0^1 {{x^\alpha }dx}  = \left. {\frac{{{x^{\alpha  + 1}}}}{{\alpha  + 1}}} \right|_0^{x = 1} = } \frac{1}{{\alpha  + 1}}\] 
Daraus folgt aus Fundamentales Satz der Integral Rechnung \[u\left( \alpha  \right) = \int {u'\left( \alpha  \right)d\alpha  = \int {\frac{{d\alpha }}{{\alpha  + 1}} = \log \left( {\alpha  + 1} \right)} }  + C\]
Für eine noch zu bestimmende Konstante $C$. Aber
\[u\left( 0 \right) = \int\limits_0^1 {0dx = 0 \Rightarrow C = 0} \]
\[ \Rightarrow \int\limits_0^1 {\frac{{{x^\alpha } - 1}}{{\log x}}dx = \log \left( {\alpha  + 1} \right)} \]
\[ \Rightarrow \int\limits_0^1 {\frac{{{x^5} - 1}}{{\log x}}dx = \log 6} \]

\subsubsection*{Beweis Satz 8.21 (Idee)}
Sei \[f\left( {x,y} \right) = \int\limits_a^x {h\left( {s,y} \right)ds}:\mathbb{R}^2\rightarrow\mathbb{R} \]
\[g\left( t \right) = \left( {\begin{array}{*{20}{c}}{b\left( t \right)}\\t\end{array}} \right):\mathbb{R}\rightarrow\mathbb{R}^2\text{, }g'\left( t \right) = \left( {\begin{array}{*{20}{c}}{b'\left( t \right)}\\1\end{array}} \right)\]
Dann \[u(t) = \left( {f \circ g} \right)(t) = f\left( {b(t),t} \right) = \int\limits_a^{b(t)} {h(s,t)ds} \]